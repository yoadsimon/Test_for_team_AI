import os
from typing import List, Optional, Dict, Any
import google.generativeai as genai
from dataclasses import dataclass
from dotenv import load_dotenv
import logging
from pydantic import BaseModel, Field
import numpy as np
import cv2
import tempfile

# LangChain imports
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser

# PIL imports with proper error handling
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False


@dataclass
class HighlightDescription:
    """Description of a video highlight generated by the LLM."""
    timestamp: float
    description: str
    summary: Optional[str] = None
    importance_score: Optional[int] = None
    category: Optional[str] = None


class HighlightOutput(BaseModel):
    """Structured output for highlight generation."""
    is_highlight: bool = Field(description="Whether this moment deserves to be a highlight")
    importance_score: int = Field(description="Importance score from 1-10")
    description: str = Field(description="Clear, engaging description of what's happening")
    category: str = Field(description="Category: action, dialogue, scene_change, key_moment, visual_action, or other")
    summary: str = Field(description="One-sentence summary of the significance")


class LLMService:
    """Enhanced LLM service using LangChain for intelligent highlight extraction with vision capabilities."""

    def __init__(self):
        """Initialize the enhanced LLM service with LangChain and vision capabilities."""
        # Load environment variables
        load_dotenv()
        
        # Get API key from environment
        self.api_key = os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY not found in environment variables. Please add it to your .env file.")
        
        try:
            # Configure the Gemini API
            genai.configure(api_key=self.api_key)
            
            # Initialize LangChain LLM
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                google_api_key=self.api_key,
                temperature=0.7
            )
            
            # Initialize Gemini Vision model for frame analysis
            self.vision_model = genai.GenerativeModel('gemini-1.5-flash')
            
            # Initialize the embedding model
            self.embedding_model = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001",
                google_api_key=self.api_key,
                task_type="retrieval_document"
            )
            
            # Set up output parser
            self.output_parser = PydanticOutputParser(pydantic_object=HighlightOutput)
            
            # Set up structured prompts
            self._setup_prompts()
            
            # Set up chains
            self._setup_chains()

            # Set up logging
            self.logger = logging.getLogger(__name__)
            self.logger.info("Enhanced LLM service with Vision and LangChain initialized successfully")
            
        except Exception as e:
            raise RuntimeError(f"Failed to initialize services: {str(e)}")

    def _setup_prompts(self):
        """Set up structured prompts for different tasks."""
        
        # Enhanced highlight filtering and generation prompt with visual context
        self.highlight_prompt = PromptTemplate(
            input_variables=["audio_text", "visual_description", "timestamp", "video_context"],
            template="""You are an expert video content analyst. Analyze this moment from a video using BOTH audio and visual information:

TIMESTAMP: {timestamp:.1f} seconds
AUDIO: "{audio_text}"
VISUAL: "{visual_description}"
VIDEO CONTEXT: {video_context}

Determine if this moment should be a highlight. Good highlights include:
- Important dialogue or key information
- Significant actions or events (even without sound)
- Scene transitions or dramatic moments
- Educational or informative content
- Emotional or engaging moments
- Visual effects or impressive cinematography
- Action sequences or movement
- Object interactions or demonstrations
- Facial expressions showing strong emotions
- Environmental changes or reveals

Avoid highlighting:
- Static scenes with filler words
- Long pauses with no visual interest
- Repetitive content (audio + visual)
- Low-value chatter with boring visuals

Consider the combination of audio and visual elements to determine significance.

{format_instructions}

Provide your analysis:""",
            partial_variables={"format_instructions": self.output_parser.get_format_instructions()}
        )
        
        # Summary generation prompt
        self.summary_prompt = PromptTemplate(
            input_variables=["highlights"],
            template="""You are summarizing a video based on its key highlights. Here are the important moments:

{highlights}

Create a concise but informative summary that:
1. Captures the main narrative or key points
2. Highlights the most important moments (both audio and visual)
3. Explains the overall context and significance
4. Uses engaging, natural language
5. Is 2-3 sentences long

Summary:"""
        )

    def _setup_chains(self):
        """Set up modern LangChain chains using Runnable interface."""
        
        # Modern highlight generation chain using pipe operator
        self.highlight_chain = self.highlight_prompt | self.llm | self.output_parser
        
        # Modern summary generation chain using pipe operator  
        self.summary_chain = self.summary_prompt | self.llm | StrOutputParser()

    def _analyze_frame(self, frame: np.ndarray) -> str:
        """
        Analyze a video frame using Gemini Vision.
        
        Args:
            frame: OpenCV frame (numpy array)
            
        Returns:
            Description of what's happening in the frame
        """
        try:
            if not PIL_AVAILABLE:
                return "PIL not available for image processing"
            
            # Convert OpenCV frame (BGR) to RGB and then to PIL Image
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Save frame to temporary file instead of using PIL directly
            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
                temp_path = temp_file.name
                
            # Save the frame using OpenCV
            cv2.imwrite(temp_path, frame)
            
            try:
                # Load with PIL for Gemini Vision
                pil_image = Image.open(temp_path)
                
                # Resize image if too large (Gemini has size limits)
                max_size = 1024
                if pil_image.width > max_size or pil_image.height > max_size:
                    pil_image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                
                # Analyze with Gemini Vision
                prompt = """Describe what you see in this video frame in 1-2 sentences. Focus on:
- Main subjects/people and what they're doing
- Important objects or actions
- Scene setting or environment
- Any significant visual elements
- Emotional expressions or body language
- Movement or transitions

Be concise but descriptive."""
                
                response = self.vision_model.generate_content([prompt, pil_image])
                return response.text.strip()
                
            finally:
                # Clean up temporary file
                try:
                    os.unlink(temp_path)
                except:
                    pass
            
        except Exception as e:
            self.logger.error(f"Error analyzing frame: {e}")
            return "Unable to analyze visual content"

    def generate_highlight_description(
        self,
        audio_context: str,
        timestamp: float,
        video_context: str = "General video content",
        frame: Optional[np.ndarray] = None
    ) -> Optional[HighlightDescription]:
        """
        Generate a highlight description using smart filtering with both audio and visual analysis.
        
        Args:
            audio_context: Transcribed audio text
            timestamp: Timestamp in seconds
            video_context: Brief context about the video content
            frame: Optional video frame for visual analysis
            
        Returns:
            HighlightDescription if moment is significant, None otherwise
        """
        try:
            # Analyze visual content if frame is provided
            if frame is not None:
                visual_description = self._analyze_frame(frame)
                self.logger.info(f"Visual analysis at {timestamp:.1f}s: {visual_description[:100]}...")
            else:
                visual_description = "No visual information available"
            
            # Use enhanced LangChain chain with both audio and visual context
            result = self.highlight_chain.invoke({
                "audio_text": audio_context,
                "visual_description": visual_description,
                "timestamp": timestamp,
                "video_context": video_context
            })
            
            # Only create highlight if the AI determines it's significant
            if result.is_highlight and result.importance_score >= 6:  # Threshold for quality
                self.logger.info(f"✨ Highlight created at {timestamp:.1f}s - Score: {result.importance_score}")
                return HighlightDescription(
                    timestamp=timestamp,
                    description=result.description,
                    summary=result.summary,
                    importance_score=result.importance_score,
                    category=result.category
                )
            else:
                self.logger.debug(f"Segment at {timestamp:.1f}s filtered out - Score: {result.importance_score}")
            
            return None  # Not significant enough to be a highlight
            
        except Exception as e:
            self.logger.error(f"Error generating highlight description: {e}")
            # Fallback to simple description
            return HighlightDescription(
                timestamp=timestamp,
                description=f"Video moment with audio: {audio_context[:100]}...",
                summary="Video content",
                importance_score=5
            )

    def generate_highlight_summary(
        self, highlights: List[HighlightDescription]
    ) -> str:
        """
        Generate a summary of multiple highlights using LangChain.
        
        Args:
            highlights: List of highlight descriptions
            
        Returns:
            Summary of the highlights
        """
        if not highlights:
            return "No significant highlights found in this video."

        # Prepare highlights for summary
        highlights_text = "\n".join([
            f"• At {h.timestamp:.1f}s: {h.description} (Score: {h.importance_score or 'N/A'})"
            for h in highlights
        ])

        try:
            # Use modern LangChain Runnable chain - this directly returns a string
            summary = self.summary_chain.invoke({"highlights": highlights_text})
            return summary.strip()
        except Exception as e:
            self.logger.error(f"Error generating summary: {e}")
            return f"Video contains {len(highlights)} key highlights covering various important moments."

    def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding for text using Google's embedding model.
        
        Args:
            text: Text to generate embedding for
            
        Returns:
            List of embedding values
        """
        try:
            embedding = self.embedding_model.embed_query(text)
            return embedding
        except Exception as e:
            self.logger.error(f"Error generating embedding: {e}")
            # Return a zero vector as fallback
            return [0.0] * 768  # Standard embedding dimension

    def batch_generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        Generate embeddings for multiple texts efficiently.
        
        Args:
            texts: List of texts to generate embeddings for
            
        Returns:
            List of embedding vectors
        """
        try:
            embeddings = self.embedding_model.embed_documents(texts)
            return embeddings
        except Exception as e:
            self.logger.error(f"Error generating batch embeddings: {e}")
            # Return zero vectors as fallback
            return [[0.0] * 768] * len(texts) 