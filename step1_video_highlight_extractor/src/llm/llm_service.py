import os
from typing import List, Optional, Dict, Any
import google.generativeai as genai
from dataclasses import dataclass
from dotenv import load_dotenv
import logging
from pydantic import BaseModel, Field
import numpy as np
import cv2
import tempfile

# LangChain imports
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.output_parsers import PydanticOutputParser
from langchain.memory import ConversationBufferMemory
from langchain_core.output_parsers import StrOutputParser

# PIL imports with proper error handling
try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False


@dataclass
class HighlightDescription:
    """Description of a video highlight generated by the LLM."""
    timestamp: float
    description: str
    summary: Optional[str] = None
    importance_score: Optional[int] = None
    category: Optional[str] = None


class HighlightOutput(BaseModel):
    """Structured output for highlight generation."""
    is_highlight: bool = Field(description="Whether this moment deserves to be a highlight")
    importance_score: int = Field(description="Importance score from 1-10")
    description: str = Field(description="Clear, engaging description of what's happening")
    category: str = Field(description="Category: action, dialogue, scene_change, key_moment, visual_action, or other")
    summary: str = Field(description="One-sentence summary of the significance")


class LLMService:
    """LLM service using LangChain for intelligent highlight extraction with vision capabilities."""

    def __init__(self):
        """Initialize the LLM service with LangChain and vision capabilities."""
        # Load environment variables
        load_dotenv()
        
        # Get API key from environment
        self.api_key = os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY not found in environment variables. Please add it to your .env file.")
        
        try:
            # Configure the Gemini API
            genai.configure(api_key=self.api_key)
            
            # Initialize LangChain LLM
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-1.5-flash",
                google_api_key=self.api_key,
                temperature=0.7
            )
            
            # Initialize Gemini Vision model for frame analysis
            self.vision_model = genai.GenerativeModel('gemini-1.5-flash')
            
            # Initialize the embedding model
            self.embedding_model = GoogleGenerativeAIEmbeddings(
                model="models/embedding-001",
                google_api_key=self.api_key,
                task_type="retrieval_document"
            )
            
            # Set up output parser
            self.output_parser = PydanticOutputParser(pydantic_object=HighlightOutput)
            
            # Set up structured prompts
            self._setup_prompts()
            
            # Set up chains
            self._setup_chains()

            # Set up logging
            self.logger = logging.getLogger(__name__)
            self.logger.info("LLM service with Vision and LangChain initialized successfully")
            
        except Exception as e:
            raise RuntimeError(f"Failed to initialize services: {str(e)}")

    def _setup_prompts(self):
        """Set up structured prompts for different tasks."""
        
        # Enhanced highlight filtering and generation prompt with visual context
        self.highlight_prompt = PromptTemplate(
            input_variables=["audio_text", "visual_description", "timestamp", "video_context"],
            template="""You are an expert video content analyst. Analyze this moment from a video using BOTH audio and visual information:

TIMESTAMP: {timestamp:.1f} seconds
AUDIO: "{audio_text}"
VISUAL: "{visual_description}"
VIDEO CONTEXT: {video_context}

Determine if this moment should be a highlight. Good highlights include:
- Important dialogue or key information
- Significant actions or events (even without sound)
- Scene transitions or dramatic moments
- Educational or informative content
- Emotional or engaging moments
- Visual effects or impressive cinematography
- Action sequences or movement
- Object interactions or demonstrations
- Facial expressions showing strong emotions
- Environmental changes or reveals

Avoid highlighting:
- Static scenes with filler words
- Long pauses with no visual interest
- Repetitive content (audio + visual)
- Low-value chatter with boring visuals

Consider the combination of audio and visual elements to determine significance.

{format_instructions}

Provide your analysis:""",
            partial_variables={"format_instructions": self.output_parser.get_format_instructions()}
        )
        
        # Summary generation prompt
        self.summary_prompt = PromptTemplate(
            input_variables=["highlights"],
            template="""You are summarizing a video based on its key highlights. Here are the important moments:

{highlights}

Create a concise but informative summary that:
1. Captures the main narrative or key points
2. Highlights the most important moments (both audio and visual)
3. Explains the overall context and significance
4. Uses engaging, natural language
5. Is 2-3 sentences long

Summary:"""
        )

    def _setup_chains(self):
        """Set up modern LangChain chains using Runnable interface."""
        
        # Modern highlight generation chain using pipe operator
        self.highlight_chain = self.highlight_prompt | self.llm | self.output_parser
        
        # Modern summary generation chain using pipe operator  
        self.summary_chain = self.summary_prompt | self.llm | StrOutputParser()

    def _analyze_frame(self, frame: np.ndarray) -> str:
        """Analyze a video frame using Gemini Vision."""
        try:
            if not PIL_AVAILABLE:
                return "PIL not available for image processing"
            
            # Convert OpenCV frame (BGR) to RGB and then to PIL Image
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Save frame to temporary file instead of using PIL directly
            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
                temp_path = temp_file.name
                
            # Save the frame using OpenCV
            cv2.imwrite(temp_path, frame)
            
            try:
                # Load with PIL for Gemini Vision
                pil_image = Image.open(temp_path)
                
                # Resize image if too large (Gemini has size limits)
                max_size = 1024
                if pil_image.width > max_size or pil_image.height > max_size:
                    pil_image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)
                
                # Analyze with Gemini Vision
                prompt = """Describe what you see in this video frame in 1-2 sentences. Focus on:
- Main subjects/people and what they're doing
- Important objects or actions
- Scene setting or environment
- Any significant visual elements
- Emotional expressions or body language
- Movement or transitions

Keep it concise and factual."""
                
                response = self.vision_model.generate_content([prompt, pil_image])
                return response.text.strip()
                
            finally:
                # Clean up temporary file
                if os.path.exists(temp_path):
                    os.remove(temp_path)
            
        except Exception as e:
            self.logger.warning(f"Frame analysis failed: {e}")
            return "Unable to analyze visual content"

    def generate_highlight_description(
        self,
        audio_context: str,
        timestamp: float,
        video_context: str = "General video content",
        frame: Optional[np.ndarray] = None
    ) -> Optional[HighlightDescription]:
        """Generate a highlight description using enhanced analysis with visual context."""
        try:
            visual_description = "No visual analysis available"
            if frame is not None:
                visual_description = self._analyze_frame(frame)
            
            result = self.highlight_chain.invoke({
                "audio_text": audio_context,
                "visual_description": visual_description,
                "timestamp": timestamp,
                "video_context": video_context
            })
            
            if result.is_highlight and result.importance_score >= 6:
                return HighlightDescription(
                    timestamp=timestamp,
                    description=result.description,
                    summary=result.summary,
                    importance_score=result.importance_score,
                    category=result.category
                )
            
            return None
            
        except Exception as e:
            self.logger.error(f"Highlight generation failed for timestamp {timestamp}: {e}")
            return None

    def generate_highlight_summary(
        self, highlights: List[HighlightDescription]
    ) -> str:
        """Generate a summary of all highlights for a video."""
        if not highlights:
            return "No significant highlights found in this video."
        
        try:
            highlights_text = "\n".join([
                f"- {h.timestamp:.1f}s: {h.description} ({h.category})"
                for h in highlights[:10]
            ])
            
            if len(highlights) > 10:
                highlights_text += f"\n... and {len(highlights) - 10} more highlights"
            
            summary = self.summary_chain.invoke({"highlights": highlights_text})
            return summary.strip()
            
        except Exception as e:
            self.logger.error(f"Summary generation failed: {e}")
            return f"Video contains {len(highlights)} notable highlights covering various topics and moments."

    def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for a text."""
        try:
            embedding = self.embedding_model.embed_query(text)
            return embedding
        except Exception as e:
            self.logger.error(f"Embedding generation failed: {e}")
            return [0.0] * 768

    def batch_generate_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Generate embeddings for multiple texts efficiently."""
        try:
            embeddings = self.embedding_model.embed_documents(texts)
            return embeddings
        except Exception as e:
            self.logger.error(f"Batch embedding generation failed: {e}")
            return [[0.0] * 768] * len(texts) 